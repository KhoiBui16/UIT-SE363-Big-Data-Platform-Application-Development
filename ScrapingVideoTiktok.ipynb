{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roHAZboBqPpI",
        "outputId": "091d4e01-1e92-41c0-a764-2e41b02906ed"
      },
      "outputs": [],
      "source": [
        "# ===========================\n",
        "# 0) C√ÄI TH∆Ø VI·ªÜN C·∫¶N THI·∫æT\n",
        "# ===========================\n",
        "!pip -q install yt-dlp requests pandas xlsxwriter\n",
        "print(\"‚úÖ ƒê√£ c√†i ƒë·∫∑t xong c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zetqkWVlq1lI",
        "outputId": "d513d9c3-0ca2-4e59-e0e3-87cfe1989798"
      },
      "outputs": [],
      "source": [
        "# ===========================\n",
        "# 1) KHAI B√ÅO & MOUNT DRIVE (tu·ª≥ ch·ªçn)\n",
        "# ===========================\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# BASE_DRIVE_DIR = \"/content/drive/MyDrive/SE363/Crawl\"   # ƒë·ªïi n·∫øu mu·ªën l∆∞u th·∫≥ng v√†o Drive c·ªßa b·∫°n\n",
        "# print(\"Output folder:\", BASE_DRIVE_DIR)\n",
        "\n",
        "import os\n",
        "\n",
        "ROOT_DIR = os.path.dirname(os.path.abspath(__file__)) if \"__file__\" in globals() else os.getcwd()\n",
        "# 2. ƒê·ªãnh nghƒ©a c√°c ƒë∆∞·ªùng d·∫´n con\n",
        "CRAWL_DIR = os.path.join(ROOT_DIR, \"data\", \"crawl\")\n",
        "VIDEO_DIR = os.path.join(ROOT_DIR, \"data\", \"videos\")\n",
        "\n",
        "# ƒê∆∞·ªùng d·∫´n file CSV ƒë·∫ßu v√†o (ƒë∆∞·ª£c t·∫°o b·ªüi script crawl)\n",
        "INPUT_CSV = os.path.join(CRAWL_DIR, \"tiktok_links.csv\")\n",
        "HARMFUL_DIR = os.path.join(VIDEO_DIR, \"harmful\")\n",
        "NOT_HARMFUL_DIR = os.path.join(VIDEO_DIR, \"not_harmful\")\n",
        "\n",
        "# 3. T·∫°o c√°c th∆∞ m·ª•c n·∫øu ch∆∞a t·ªìn t·∫°i\n",
        "os.makedirs(CRAWL_DIR, exist_ok=True)\n",
        "os.makedirs(VIDEO_DIR, exist_ok=True)\n",
        "os.makedirs(HARMFUL_DIR, exist_ok=True)\n",
        "os.makedirs(NOT_HARMFUL_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "print(f\"üìÇ Th∆∞ m·ª•c d·ª± √°n: {ROOT_DIR}\")\n",
        "print(f\"üìÑ File d·ªØ li·ªáu ƒë·∫ßu v√†o: {INPUT_CSV}\")\n",
        "print(f\"üìÅ Th∆∞ m·ª•c l∆∞u video HARMFUL: {HARMFUL_DIR}\")\n",
        "print(f\"üìÅ Th∆∞ m·ª•c l∆∞u video NOT_HARMFUL: {NOT_HARMFUL_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "bIH_mAeEq2xh",
        "outputId": "820a6c1e-455a-4f88-dc37-6eb2e7eb950d"
      },
      "outputs": [],
      "source": [
        "# ===========================\n",
        "# 2) IMPORT & N·∫†P COOKIES.TXT (Netscape)\n",
        "#    -> KH√îNG g√°n th·∫≥ng v√†o header \"Cookie\"\n",
        "#    -> D√πng MozillaCookieJar + requests.Session\n",
        "# ===========================\n",
        "import os, re, json, time, requests\n",
        "import http.cookiejar as cookielib\n",
        "from urllib.parse import urlparse\n",
        "from datetime import datetime, timezone, timedelta\n",
        "import pandas as pd\n",
        "import yt_dlp\n",
        "\n",
        "COOKIES_PATH = \"cookies.txt\"  # Upload file n√†y l√™n Colab c√πng notebook\n",
        "\n",
        "# Ki·ªÉm tra cookies.txt t·ªìn t·∫°i\n",
        "if not os.path.exists(COOKIES_PATH):\n",
        "    from google.colab import files\n",
        "    print(\"Vui l√≤ng upload cookies.txt (Export t·ª´ tr√¨nh duy·ªát).\")\n",
        "    uploaded = files.upload()\n",
        "    COOKIES_PATH = next(iter(uploaded))\n",
        "\n",
        "# T·∫°o session d√πng cookie Netscape\n",
        "cj = cookielib.MozillaCookieJar()\n",
        "cj.load(COOKIES_PATH, ignore_discard=True, ignore_expires=True)\n",
        "\n",
        "session = requests.Session()\n",
        "session.cookies = cj\n",
        "session.headers.update({\n",
        "    \"User-Agent\": \"Mozilla/5.0\",\n",
        "    \"Referer\": \"https://www.tiktok.com/\"\n",
        "})\n",
        "\n",
        "# Ki·ªÉm tra c√≥ cookie cho tiktok kh√¥ng\n",
        "has_tiktok_cookies = any(\"tiktok.com\" in c.domain for c in session.cookies)\n",
        "print(\"Cookies for tiktok.com found:\", has_tiktok_cookies)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kmwb6AFSq8SU"
      },
      "outputs": [],
      "source": [
        "# ===========================\n",
        "# 3) H√ÄM H·ªñ TR·ª¢ (oEmbed, parse URL/aweme_id)\n",
        "# ===========================\n",
        "def fetch_oembed(video_url, timeout=12):\n",
        "    \"\"\"L·∫•y metadata oEmbed c∆° b·∫£n c·ªßa video (official).\"\"\"\n",
        "    oembed_url = \"https://www.tiktok.com/oembed\"\n",
        "    try:\n",
        "        r = session.get(oembed_url, params={\"url\": video_url}, timeout=timeout)\n",
        "        if r.status_code == 200:\n",
        "            return r.json()\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(\"oEmbed error:\", e)\n",
        "        return None\n",
        "\n",
        "def extract_aweme_id_from_url(video_url: str):\n",
        "    \"\"\"Parse aweme_id t·ª´ URL d·∫°ng /video/<digits>\"\"\"\n",
        "    m = re.search(r\"/video/(\\d+)\", video_url)\n",
        "    return m.group(1) if m else None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZMq0CrpsEvs"
      },
      "outputs": [],
      "source": [
        "# ===========================\n",
        "# 4) DOWNLOAD VIDEO B·∫∞NG yt-dlp (d√πng cookiefile + headers)\n",
        "# ===========================\n",
        "def download_tiktok_with_yt_dlp(video_url, out_dir, write_info_json=True, max_filesize=None):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    out_template = os.path.join(out_dir, \"%(id)s.%(ext)s\")\n",
        "\n",
        "    ydl_opts = {\n",
        "        \"outtmpl\": out_template,\n",
        "        \"format\": \"best\",\n",
        "        \"noplaylist\": True,\n",
        "        # D√πng cookiefile + UA/Referer ƒë·ªÉ gi·∫£m l·ªói regional/age/403\n",
        "        \"cookiefile\": COOKIES_PATH,\n",
        "        \"http_headers\": {\n",
        "            \"User-Agent\": \"Mozilla/5.0\",\n",
        "            \"Referer\": video_url\n",
        "        },\n",
        "        # Tr√°nh t·∫£i l·∫°i c√πng video\n",
        "        \"download_archive\": os.path.join(out_dir, \"downloaded.txt\"),\n",
        "    }\n",
        "\n",
        "    if write_info_json:\n",
        "        ydl_opts.update({\n",
        "            \"writedescription\": True,\n",
        "            \"writesubtitles\": False,\n",
        "            \"writeinfojson\": True\n",
        "        })\n",
        "\n",
        "    if max_filesize:\n",
        "        ydl_opts[\"max_filesize\"] = max_filesize\n",
        "\n",
        "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "        info = ydl.extract_info(video_url, download=True)\n",
        "\n",
        "    # L·∫•y info json n·∫øu c√≥\n",
        "    info_json_path = None\n",
        "    if isinstance(info, dict) and \"id\" in info:\n",
        "        candidate = os.path.join(out_dir, f\"{info['id']}.info.json\")\n",
        "        if os.path.exists(candidate):\n",
        "            info_json_path = candidate\n",
        "\n",
        "    if info_json_path:\n",
        "        with open(info_json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            info_data = json.load(f)\n",
        "    else:\n",
        "        info_data = info\n",
        "    return info_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGcuiIxEsGsd"
      },
      "outputs": [],
      "source": [
        "# ===========================\n",
        "# 5) CRAWL COMMENT (WEB API KH√îNG CH√çNH TH·ª®C) B·∫∞NG session\n",
        "#    L∆∞u √Ω: C√≥ th·ªÉ 403/empty tr√™n Colab d√π c√≥ cookies.\n",
        "# ===========================\n",
        "def fetch_comments_web(aweme_id: str, session: requests.Session, max_comments=200, sleep=1.0):\n",
        "    url = \"https://www.tiktok.com/api/comment/list/\"\n",
        "    cursor, out = 0, []\n",
        "    while len(out) < max_comments:\n",
        "        params = {\"aid\": 1988, \"aweme_id\": aweme_id, \"cursor\": cursor, \"count\": 20}\n",
        "        r = session.get(url, params=params, timeout=20)\n",
        "        if r.status_code != 200:\n",
        "            print(\"HTTP\", r.status_code, r.text[:200])\n",
        "            break\n",
        "        try:\n",
        "            data = r.json()\n",
        "        except Exception as e:\n",
        "            print(\"JSON parse error:\", e, r.text[:200]); break\n",
        "\n",
        "        out.extend(data.get(\"comments\") or [])\n",
        "        print(f\"Fetched {len(out)} comments so far‚Ä¶\")\n",
        "\n",
        "        if not data.get(\"has_more\"):\n",
        "            break\n",
        "        cursor = data.get(\"cursor\", cursor + 20)\n",
        "        if sleep: time.sleep(sleep)\n",
        "    return out\n",
        "\n",
        "def fetch_replies_web(aweme_id: str, comment_id: str, session: requests.Session, max_replies=100, sleep=1.0):\n",
        "    url = \"https://www.tiktok.com/api/comment/list/reply/\"\n",
        "    cursor, out = 0, []\n",
        "    while len(out) < max_replies:\n",
        "        params = {\"aid\": 1988, \"aweme_id\": aweme_id, \"comment_id\": comment_id, \"cursor\": cursor, \"count\": 20}\n",
        "        r = session.get(url, params=params, timeout=20)\n",
        "        if r.status_code != 200:\n",
        "            print(\"HTTP\", r.status_code, r.text[:200])\n",
        "            break\n",
        "        try:\n",
        "            data = r.json()\n",
        "        except Exception as e:\n",
        "            print(\"JSON parse error:\", e, r.text[:200]); break\n",
        "\n",
        "        out.extend(data.get(\"comments\") or [])\n",
        "        if not data.get(\"has_more\"):\n",
        "            break\n",
        "        cursor = data.get(\"cursor\", cursor + 20)\n",
        "        if sleep: time.sleep(sleep)\n",
        "    return out\n",
        "\n",
        "def fetch_all_comments_with_replies_web(aweme_id: str, session: requests.Session,\n",
        "                                        max_comments=200, max_replies_per_comment=50, sleep=1.0):\n",
        "    comments = fetch_comments_web(aweme_id, session, max_comments=max_comments, sleep=sleep)\n",
        "    results = []\n",
        "    for c in comments:\n",
        "        item = {\n",
        "            \"cid\": c.get(\"cid\"),\n",
        "            \"text\": c.get(\"text\"),\n",
        "            \"author\": (c.get(\"user\") or {}).get(\"nickname\"),\n",
        "            \"create_time\": c.get(\"create_time\"),\n",
        "            \"like_count\": c.get(\"digg_count\"),\n",
        "            \"reply_count\": c.get(\"reply_comment_total\"),\n",
        "            \"replies\": []\n",
        "        }\n",
        "        try:\n",
        "            if item[\"cid\"]:\n",
        "                rs = fetch_replies_web(aweme_id, item[\"cid\"], session, max_replies=max_replies_per_comment, sleep=sleep)\n",
        "                item[\"replies\"] = rs\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Reply error for {item['cid']}: {e}\")\n",
        "        results.append(item)\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vs3qqXbcsIiM"
      },
      "outputs": [],
      "source": [
        "# ===========================\n",
        "# 6) H√ÄM CRAWL 1 VIDEO: oEmbed ‚Üí yt-dlp ‚Üí comments ‚Üí JSON\n",
        "#    + Fallback l·∫•y aweme_id n·∫øu oEmbed kh√¥ng c√≥\n",
        "# ===========================\n",
        "def crawl_one_tiktok(video_url, out_dir, use_comments=False, max_comments=200, max_replies_per_comment=50, sleep=1.0):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    result = {\n",
        "        \"url\": video_url,\n",
        "        \"crawled_at\": datetime.utcnow().isoformat() + \"Z\"\n",
        "    }\n",
        "\n",
        "    # 1) oEmbed\n",
        "    oembed = fetch_oembed(video_url)\n",
        "    result[\"oembed\"] = oembed\n",
        "\n",
        "    # 2) T·∫£i video + ƒë·ªçc info\n",
        "    try:\n",
        "        meta = download_tiktok_with_yt_dlp(video_url, out_dir)\n",
        "        result[\"yt_dlp_info\"] = meta\n",
        "    except Exception as e:\n",
        "        result[\"yt_dlp_error\"] = str(e)\n",
        "        meta = None\n",
        "\n",
        "    # 3) L·∫•y aweme_id\n",
        "    aweme_id = None\n",
        "    if oembed and isinstance(oembed, dict):\n",
        "        aweme_id = oembed.get(\"embed_product_id\")\n",
        "    if not aweme_id:\n",
        "        aweme_id = extract_aweme_id_from_url(video_url)\n",
        "    if not aweme_id and isinstance(meta, dict):\n",
        "        aweme_id = str(meta.get(\"id\"))\n",
        "\n",
        "    if not aweme_id:\n",
        "        raise ValueError(\"Kh√¥ng l·∫•y ƒë∆∞·ª£c aweme_id cho video n√†y\")\n",
        "\n",
        "    print(\"aweme_id:\", aweme_id)\n",
        "\n",
        "    # 4) Comments (tu·ª≥ ch·ªçn)\n",
        "    if use_comments:\n",
        "        cmts = fetch_all_comments_with_replies_web(\n",
        "            aweme_id, session, max_comments=max_comments,\n",
        "            max_replies_per_comment=max_replies_per_comment, sleep=sleep\n",
        "        )\n",
        "        result[\"comments\"] = cmts\n",
        "\n",
        "    # 5) Save metadata JSON\n",
        "    json_path = os.path.join(out_dir, f\"{aweme_id}_crawl.json\")\n",
        "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(result, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    return result, json_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===========================\n",
        "# 7) XU·∫§T EXCEL (comments + replies th√†nh 1 sheet)\n",
        "# ===========================\n",
        "# def export_comments_to_excel(all_items, aweme_id: str, out_dir: str):\n",
        "#     rows = []\n",
        "#     for c in all_items or []:\n",
        "#         # Top-level\n",
        "#         rows.append({\n",
        "#             \"video_id\": aweme_id,\n",
        "#             \"cid\": c.get(\"cid\"),\n",
        "#             \"parent_cid\": None,\n",
        "#             \"is_reply\": 0,\n",
        "#             \"author_name\": c.get(\"author\"),\n",
        "#             \"text\": c.get(\"text\"),\n",
        "#             \"like_count\": c.get(\"like_count\"),\n",
        "#             \"reply_count\": c.get(\"reply_count\"),\n",
        "#             \"create_time\": c.get(\"create_time\"),\n",
        "#         })\n",
        "#         # Replies\n",
        "#         for r in (c.get(\"replies\") or []):\n",
        "#             ru = r.get(\"user\") or {}\n",
        "#             rows.append({\n",
        "#                 \"video_id\": aweme_id,\n",
        "#                 \"cid\": r.get(\"cid\"),\n",
        "#                 \"parent_cid\": c.get(\"cid\"),\n",
        "#                 \"is_reply\": 1,\n",
        "#                 \"author_name\": ru.get(\"nickname\"),\n",
        "#                 \"text\": r.get(\"text\"),\n",
        "#                 \"like_count\": r.get(\"digg_count\") or r.get(\"like_count\"),\n",
        "#                 \"reply_count\": r.get(\"reply_comment_total\") or r.get(\"reply_count\"),\n",
        "#                 \"create_time\": r.get(\"create_time\"),\n",
        "#             })\n",
        "\n",
        "#     df = pd.DataFrame(rows)\n",
        "\n",
        "#     # Epoch -> th·ªùi gian\n",
        "#     if \"create_time\" in df.columns:\n",
        "#         dt = pd.to_datetime(df[\"create_time\"], unit=\"s\", errors=\"coerce\", utc=True)\n",
        "#         df[\"created_at_utc\"] = dt\n",
        "#         df[\"created_at_vn\"] = dt.dt.tz_convert(\"Asia/Ho_Chi_Minh\")\n",
        "\n",
        "#     # S·∫Øp c·ªôt\n",
        "#     cols = [\"video_id\",\"cid\",\"parent_cid\",\"is_reply\",\"author_name\",\n",
        "#             \"text\",\"like_count\",\"reply_count\",\"create_time\",\n",
        "#             \"created_at_utc\",\"created_at_vn\"]\n",
        "#     df = df[[c for c in cols if c in df.columns]]\n",
        "\n",
        "#     os.makedirs(out_dir, exist_ok=True)\n",
        "#     xlsx_path = os.path.join(out_dir, f\"{aweme_id}_comments.xlsx\")\n",
        "#     with pd.ExcelWriter(xlsx_path, engine=\"xlsxwriter\") as writer:\n",
        "#         df.to_excel(writer, index=False, sheet_name=\"comments\")\n",
        "#         ws = writer.sheets[\"comments\"]\n",
        "#         ws.freeze_panes(1, 0)\n",
        "#         for i, col in enumerate(df.columns):\n",
        "#             maxlen = min(60, max(10, df[col].astype(str).str.len().max() if not df.empty else 10))\n",
        "#             ws.set_column(i, i, maxlen + 2)\n",
        "#     print(\"‚úÖ Saved Excel:\", xlsx_path)\n",
        "#     return xlsx_path, df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgyAjyq3sM_Z"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def export_comments_to_excel(all_items, aweme_id: str, out_dir: str):\n",
        "    import os, pandas as pd\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    rows = []\n",
        "    for c in all_items or []:\n",
        "        # Top-level\n",
        "        rows.append({\n",
        "            \"video_id\": aweme_id,\n",
        "            \"cid\": c.get(\"cid\"),\n",
        "            \"parent_cid\": None,\n",
        "            \"is_reply\": 0,\n",
        "            \"author_name\": c.get(\"author\"),\n",
        "            \"text\": c.get(\"text\"),\n",
        "            \"like_count\": c.get(\"like_count\"),\n",
        "            \"reply_count\": c.get(\"reply_count\"),\n",
        "            \"create_time\": c.get(\"create_time\"),\n",
        "        })\n",
        "        # Replies\n",
        "        for r in (c.get(\"replies\") or []):\n",
        "            ru = r.get(\"user\") or {}\n",
        "            rows.append({\n",
        "                \"video_id\": aweme_id,\n",
        "                \"cid\": r.get(\"cid\"),\n",
        "                \"parent_cid\": c.get(\"cid\"),\n",
        "                \"is_reply\": 1,\n",
        "                \"author_name\": ru.get(\"nickname\"),\n",
        "                \"text\": r.get(\"text\"),\n",
        "                \"like_count\": r.get(\"digg_count\") or r.get(\"like_count\"),\n",
        "                \"reply_count\": r.get(\"reply_comment_total\") or r.get(\"reply_count\"),\n",
        "                \"create_time\": r.get(\"create_time\"),\n",
        "            })\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "\n",
        "    # Epoch -> datetime (tz-aware), r·ªìi b·ªè tz ƒë·ªÉ Excel ch·∫•p nh·∫≠n\n",
        "    if \"create_time\" in df.columns:\n",
        "        dt_utc = pd.to_datetime(df[\"create_time\"], unit=\"s\", errors=\"coerce\", utc=True)\n",
        "        # B·∫¢N NAIVE (kh√¥ng timezone)\n",
        "        df[\"created_at_utc\"] = dt_utc.dt.tz_localize(None)\n",
        "        df[\"created_at_vn\"]  = dt_utc.dt.tz_convert(\"Asia/Ho_Chi_Minh\").dt.tz_localize(None)\n",
        "\n",
        "    # S·∫Øp c·ªôt\n",
        "    cols = [\"video_id\",\"cid\",\"parent_cid\",\"is_reply\",\"author_name\",\n",
        "            \"text\",\"like_count\",\"reply_count\",\"create_time\",\n",
        "            \"created_at_utc\",\"created_at_vn\"]\n",
        "    df = df[[c for c in cols if c in df.columns]]\n",
        "\n",
        "    xlsx_path = os.path.join(out_dir, f\"{aweme_id}_comments.xlsx\")\n",
        "    with pd.ExcelWriter(xlsx_path, engine=\"xlsxwriter\",\n",
        "                        datetime_format=\"yyyy-mm-dd hh:mm:ss\") as writer:\n",
        "        df.to_excel(writer, index=False, sheet_name=\"comments\")\n",
        "        ws = writer.sheets[\"comments\"]\n",
        "        ws.freeze_panes(1, 0)\n",
        "        # Auto-width\n",
        "        for i, col in enumerate(df.columns):\n",
        "            maxlen = min(60, max(10, df[col].astype(str).str.len().max() if not df.empty else 10))\n",
        "            ws.set_column(i, i, maxlen + 2)\n",
        "\n",
        "    print(\"‚úÖ Saved Excel:\", xlsx_path)\n",
        "    return xlsx_path, df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===========================\n",
        "# 8) CH·∫†Y T·ª∞ ƒê·ªòNG T·ª™ FILE CSV (C√ì PH√ÇN LO·∫†I & RESUME)\n",
        "# ===========================\n",
        "import pandas as pd\n",
        "import time\n",
        "import os\n",
        "# from tqdm.notebook import tqdm # B·ªè comment n·∫øu mu·ªën d√πng thanh ti·∫øn tr√¨nh ƒë·∫πp h∆°n\n",
        "\n",
        "# 1. ƒê·ªçc v√† L·ªçc file CSV\n",
        "if not os.path.exists(INPUT_CSV):\n",
        "    print(f\"‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y file {INPUT_CSV}\")\n",
        "    print(\"Vui l√≤ng ch·∫°y script 'find_tiktok_links.py' tr∆∞·ªõc ƒë·ªÉ c√≥ d·ªØ li·ªáu.\")\n",
        "else:\n",
        "    print(f\"ƒêang ƒë·ªçc d·ªØ li·ªáu t·ª´: {INPUT_CSV} ...\")\n",
        "    df = pd.read_csv(INPUT_CSV)\n",
        "    print(f\"-> T·ªïng s·ªë d√≤ng trong CSV: {len(df)}\")\n",
        "\n",
        "    # L·ªåC QUAN TR·ªåNG: Ch·ªâ l·∫•y nh·ªØng link l√† VIDEO (c√≥ ch·ª©a '/video/')\n",
        "    # Lo·∫°i b·ªè c√°c link profile (v√≠ d·ª•: tiktok.com/@username)\n",
        "    df_videos = df[df['link'].str.contains('/video/', na=False)].copy()\n",
        "\n",
        "    # L√†m s·∫°ch link (b·ªè c√°c ph·∫ßn th·ª´a sau d·∫•u ?)\n",
        "    df_videos['link'] = df_videos['link'].apply(lambda x: x.split('?')[0])\n",
        "\n",
        "    # Lo·∫°i b·ªè c√°c link tr√πng l·∫∑p\n",
        "    df_videos = df_videos.drop_duplicates(subset=['link'])\n",
        "\n",
        "    print(f\"-> S·ªë l∆∞·ª£ng VIDEO th·ª±c t·∫ø c·∫ßn crawl: {len(df_videos)}\")\n",
        "    print(\"=======================================================\")\n",
        "\n",
        "    # 2. V√≤ng l·∫∑p Crawl ch√≠nh\n",
        "    # (D√πng itertuples ƒë·ªÉ l·∫∑p qua dataframe nhanh h∆°n)\n",
        "    for row in df_videos.itertuples(index=True):\n",
        "        idx = row.Index\n",
        "        original_url = row.link\n",
        "        label = row.label\n",
        "        hashtag = row.hashtag\n",
        "\n",
        "        print(f\"\\n‚ñ∂Ô∏è [{idx+1}/{len(df_videos)}] ƒêang x·ª≠ l√Ω: {original_url}\")\n",
        "        print(f\"   üè∑Ô∏è Nh√£n: {label.upper()} | #Ô∏è‚É£ Hashtag: #{hashtag}\")\n",
        "\n",
        "        # a) X√°c ƒë·ªãnh th∆∞ m·ª•c g·ªëc d·ª±a tr√™n nh√£n\n",
        "        if label == 'harmful':\n",
        "            base_save_dir = HARMFUL_DIR\n",
        "        else:\n",
        "            base_save_dir = NOT_HARMFUL_DIR\n",
        "\n",
        "        # b) L·∫•y ID video ƒë·ªÉ t·∫°o th∆∞ m·ª•c con ri√™ng bi·ªát\n",
        "        # (D√πng l·∫°i h√†m helper extract_aweme_id_from_url c·ªßa b·∫°n)\n",
        "        aweme_id = extract_aweme_id_from_url(original_url)\n",
        "        if not aweme_id:\n",
        "            print(\"   ‚ö†Ô∏è Kh√¥ng l·∫•y ƒë∆∞·ª£c ID video t·ª´ URL. B·ªè qua.\")\n",
        "            continue\n",
        "\n",
        "        # c) T·∫°o th∆∞ m·ª•c ri√™ng cho video n√†y\n",
        "        # C·∫•u tr√∫c: data/videos/harmful/{video_id}/\n",
        "        video_specific_dir = os.path.join(base_save_dir, aweme_id)\n",
        "\n",
        "        # d) Ki·ªÉm tra RESUME (n·∫øu th∆∞ m·ª•c ƒë√£ c√≥ v√† c√≥ file b√™n trong -> b·ªè qua)\n",
        "        if os.path.exists(video_specific_dir) and os.listdir(video_specific_dir):\n",
        "            print(f\"   ‚è≠Ô∏è Th∆∞ m·ª•c '{aweme_id}' ƒë√£ t·ªìn t·∫°i d·ªØ li·ªáu. B·ªè qua (Resume).\")\n",
        "            continue\n",
        "\n",
        "        # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥\n",
        "        os.makedirs(video_specific_dir, exist_ok=True)\n",
        "\n",
        "        # e) B·∫Øt ƒë·∫ßu Crawl (G·ªçi l·∫°i h√†m crawl_one_tiktok c·ªßa b·∫°n)\n",
        "        try:\n",
        "            print(f\"   ‚¨áÔ∏è B·∫Øt ƒë·∫ßu crawl v√†o: {video_specific_dir} ...\")\n",
        "\n",
        "            # --- G·ªåI H√ÄM CRAWL G·ªêC ---\n",
        "            res, jsonp = crawl_one_tiktok(\n",
        "                original_url,\n",
        "                video_specific_dir,\n",
        "                use_comments=True,          # L·∫•y comment\n",
        "                max_comments=200,           # TƒÉng s·ªë l∆∞·ª£ng n·∫øu c·∫ßn\n",
        "                max_replies_per_comment=50,\n",
        "                sleep=1.0                   # Ngh·ªâ gi·ªØa c√°c l·∫ßn g·ªçi API comment\n",
        "            )\n",
        "            print(f\"   ‚úÖ Metadata JSON saved: {os.path.basename(jsonp)}\")\n",
        "\n",
        "            # --- XU·∫§T EXCEL COMMENT ---\n",
        "            cmts = res.get(\"comments\", [])\n",
        "            if cmts:\n",
        "                # G·ªçi h√†m export_comments_to_excel g·ªëc\n",
        "                xlsx_path, _ = export_comments_to_excel(cmts, aweme_id, video_specific_dir)\n",
        "                if xlsx_path:\n",
        "                    print(f\"   ‚úÖ Comments Excel saved: {os.path.basename(xlsx_path)} ({len(cmts)} cmts)\")\n",
        "            else:\n",
        "                print(\"   ‚ÑπÔ∏è Kh√¥ng t√¨m th·∫•y comment n√†o.\")\n",
        "\n",
        "        except Exception as e:\n",
        "             print(f\"   ‚ùå L·ªñI KHI CRAWL VIDEO N√ÄY: {e}\")\n",
        "             # C√≥ th·ªÉ x√≥a th∆∞ m·ª•c l·ªói n·∫øu mu·ªën s·∫°ch s·∫Ω\n",
        "             # import shutil\n",
        "             # shutil.rmtree(video_specific_dir, ignore_errors=True)\n",
        "\n",
        "        # f) Ngh·ªâ ng∆°i ƒë·ªÉ tr√°nh b·ªã ch·∫∑n (Rate Limit)\n",
        "        print(\"   üí§ ƒêang ngh·ªâ 5s...\")\n",
        "        time.sleep(5)\n",
        "\n",
        "    print(\"\\nüéâüéâüéâ ƒê√É HO√ÄN T·∫§T TO√ÄN B·ªò DANH S√ÅCH VIDEO! üéâüéâüéâ\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVKV4yRAsR1V",
        "outputId": "feefc078-a1e7-42bf-b5c8-91a01dd0d16a"
      },
      "outputs": [],
      "source": [
        "# # ===========================\n",
        "# # 8) CH·∫†Y T·ª∞ ƒê·ªòNG T·ª™ FILE TXT (C√ì C∆† CH·∫æ RESUME)\n",
        "# # ===========================\n",
        "# import time\n",
        "# import os # ƒê·∫£m b·∫£o os ƒë√£ ƒë∆∞·ª£c import (m·∫∑c d√π ƒë√£ c√≥ ·ªü √¥ 3)\n",
        "\n",
        "# URL_FILE_PATH = \"urls.txt\" # T√™n file b·∫°n ƒë√£ upload\n",
        "\n",
        "# # --- ƒê·ªçc file urls.txt ---\n",
        "# try:\n",
        "#     with open(URL_FILE_PATH, 'r') as f:\n",
        "#         VIDEO_URL_LIST = [line.strip() for line in f if line.strip() and line.strip().startswith(\"http\")]\n",
        "    \n",
        "#     if not VIDEO_URL_LIST:\n",
        "#         print(f\"L·ªói: File {URL_FILE_PATH} tr·ªëng ho·∫∑c kh√¥ng ch·ª©a link h·ª£p l·ªá.\")\n",
        "#     else:\n",
        "#         print(f\"*** ƒê√£ t√¨m th·∫•y {len(VIDEO_URL_LIST)} URLs trong file. B·∫Øt ƒë·∫ßu crawl... ***\")\n",
        "\n",
        "# except FileNotFoundError:\n",
        "#     print(f\"L·ªói: Kh√¥ng t√¨m th·∫•y file {URL_FILE_PATH}. B·∫°n ƒë√£ upload file ch∆∞a?\")\n",
        "#     VIDEO_URL_LIST = []\n",
        "# # -----------------------------\n",
        "\n",
        "\n",
        "# # D√πng enumerate ƒë·ªÉ l·∫•y c·∫£ index (0, 1, 2...) v√† url\n",
        "# for index, url in enumerate(VIDEO_URL_LIST):\n",
        "    \n",
        "#     # T·∫°o t√™n th∆∞ m·ª•c, v√≠ d·ª•: \"vid_1\", \"vid_2\"...\n",
        "#     folder_name = f\"vid_{index + 1}\"\n",
        "    \n",
        "#     # T·∫°o ƒë∆∞·ªùng d·∫´n ƒë·∫ßy ƒë·ªß ƒë·∫øn th∆∞ m·ª•c m·ªõi n√†y\n",
        "#     video_specific_dir = os.path.join(CRAWL_DIR, folder_name)\n",
        "    \n",
        "#     # -------- C∆† CH·∫æ RESUME (PH·∫¶N TH√äM M·ªöI) --------\n",
        "#     # Ki·ªÉm tra xem th∆∞ m·ª•c n√†y ƒë√£ t·ªìn t·∫°i hay ch∆∞a\n",
        "#     if os.path.exists(video_specific_dir):\n",
        "#         # N·∫øu ƒë√£ t·ªìn t·∫°i, in th√¥ng b√°o v√† b·ªè qua (continue)\n",
        "#         print(f\"\\n--- [B·ªé QUA] Th∆∞ m·ª•c '{folder_name}' ƒë√£ t·ªìn t·∫°i. Chuy·ªÉn sang video ti·∫øp theo. ---\")\n",
        "#         continue # L·ªánh m·∫•u ch·ªët: D·ª´ng v√≤ng l·∫∑p n√†y, ƒëi ƒë·∫øn video ti·∫øp theo\n",
        "#     # -----------------------------------------------\n",
        "\n",
        "#     # N·∫øu th∆∞ m·ª•c ch∆∞a t·ªìn t·∫°i, T·∫†O M·ªöI v√† b·∫Øt ƒë·∫ßu crawl\n",
        "#     os.makedirs(video_specific_dir, exist_ok=True) \n",
        "\n",
        "#     print(f\"\\n=======================================================\")\n",
        "#     print(f\"--- [B·∫ÆT ƒê·∫¶U] X·ª≠ l√Ω: {url} (L∆∞u v√†o: {video_specific_dir}) ---\")\n",
        "    \n",
        "#     try:\n",
        "#         # G·ªçi h√†m crawl v√† TRUY·ªÄN TH∆Ø M·ª§C M·ªöI v√†o:\n",
        "#         res, jsonp = crawl_one_tiktok(\n",
        "#             url, video_specific_dir, \n",
        "#             use_comments=True,         \n",
        "#             max_comments=100,          \n",
        "#             max_replies_per_comment=50 \n",
        "#         )\n",
        "#         print(f\"‚úÖ ƒê√£ l∆∞u Metadata JSON v√†o: {jsonp}\")\n",
        "\n",
        "#         # N·∫øu c√≥ comments th√¨ xu·∫•t Excel\n",
        "#         cmts = res.get(\"comments\", [])\n",
        "#         if cmts:\n",
        "#             aweme_id = extract_aweme_id_from_url(url) or (res.get(\"oembed\") or {}).get(\"embed_product_id\")\n",
        "#             if aweme_id:\n",
        "#                 xlsx_path, df_preview = export_comments_to_excel(cmts, aweme_id, video_specific_dir) \n",
        "#                 print(f\"‚úÖ ƒê√£ l∆∞u Comments Excel v√†o: {xlsx_path}\")\n",
        "#             else:\n",
        "#                  print(\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y aweme_id, b·ªè qua xu·∫•t Excel.\")\n",
        "#         else:\n",
        "#             print(\"‚ÑπÔ∏è Video n√†y kh√¥ng c√≥ comment (ho·∫∑c crawl comment th·∫•t b·∫°i).\")\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"!!!!!!!! L·ªñI NGHI√äM TR·ªåNG !!!!!!!!\")\n",
        "#         print(f\"L·ªói khi x·ª≠ l√Ω {url}: {e}\")\n",
        "#         print(f\"--- [B·ªé QUA] Video n√†y v√† ti·∫øp t·ª•c. ---\")\n",
        "\n",
        "#     print(f\"=======================================================\")\n",
        "    \n",
        "#     time.sleep(5)\n",
        "\n",
        "# print(\"\\n*** ƒê√É HO√ÄN T·∫§T TO√ÄN B·ªò DANH S√ÅCH! ***\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "SE363",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
