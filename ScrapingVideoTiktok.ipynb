{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roHAZboBqPpI",
        "outputId": "091d4e01-1e92-41c0-a764-2e41b02906ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ ƒê√£ c√†i ƒë·∫∑t xong c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt.\n"
          ]
        }
      ],
      "source": [
        "# ===========================\n",
        "# 0) C√ÄI TH∆Ø VI·ªÜN C·∫¶N THI·∫æT\n",
        "# ===========================\n",
        "!pip -q install yt-dlp requests pandas xlsxwriter\n",
        "print(\"‚úÖ ƒê√£ c√†i ƒë·∫∑t xong c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zetqkWVlq1lI",
        "outputId": "d513d9c3-0ca2-4e59-e0e3-87cfe1989798"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÇ Th∆∞ m·ª•c d·ª± √°n: /home/guest/Projects/SE363/-UIT-_SE363-Big-Data-Platform-Application-Development\n",
            "üìÑ File d·ªØ li·ªáu ƒë·∫ßu v√†o: /home/guest/Projects/SE363/-UIT-_SE363-Big-Data-Platform-Application-Development/data/crawl/sub_tiktok_links.csv\n",
            "üìÅ Th∆∞ m·ª•c l∆∞u video HARMFUL: /home/guest/Projects/SE363/-UIT-_SE363-Big-Data-Platform-Application-Development/data/videos/harmful\n",
            "üìÅ Th∆∞ m·ª•c l∆∞u video NOT_HARMFUL: /home/guest/Projects/SE363/-UIT-_SE363-Big-Data-Platform-Application-Development/data/videos/not_harmful\n"
          ]
        }
      ],
      "source": [
        "# ===========================\n",
        "# 1) KHAI B√ÅO & MOUNT DRIVE (tu·ª≥ ch·ªçn)\n",
        "# ===========================\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# BASE_DRIVE_DIR = \"/content/drive/MyDrive/SE363/Crawl\"   # ƒë·ªïi n·∫øu mu·ªën l∆∞u th·∫≥ng v√†o Drive c·ªßa b·∫°n\n",
        "# print(\"Output folder:\", BASE_DRIVE_DIR)\n",
        "\n",
        "import os\n",
        "\n",
        "ROOT_DIR = os.path.dirname(os.path.abspath(__file__)) if \"__file__\" in globals() else os.getcwd()\n",
        "# 2. ƒê·ªãnh nghƒ©a c√°c ƒë∆∞·ªùng d·∫´n con\n",
        "CRAWL_DIR = os.path.join(ROOT_DIR, \"data\", \"crawl\")\n",
        "VIDEO_DIR = os.path.join(ROOT_DIR, \"data\", \"videos\")\n",
        "\n",
        "# ƒê∆∞·ªùng d·∫´n file CSV ƒë·∫ßu v√†o (ƒë∆∞·ª£c t·∫°o b·ªüi script crawl)\n",
        "INPUT_CSV = os.path.join(CRAWL_DIR, \"sub_tiktok_links.csv\")\n",
        "HARMFUL_DIR = os.path.join(VIDEO_DIR, \"harmful\")\n",
        "NOT_HARMFUL_DIR = os.path.join(VIDEO_DIR, \"not_harmful\")\n",
        "\n",
        "# 3. T·∫°o c√°c th∆∞ m·ª•c n·∫øu ch∆∞a t·ªìn t·∫°i\n",
        "os.makedirs(CRAWL_DIR, exist_ok=True)\n",
        "os.makedirs(VIDEO_DIR, exist_ok=True)\n",
        "os.makedirs(HARMFUL_DIR, exist_ok=True)\n",
        "os.makedirs(NOT_HARMFUL_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "print(f\"üìÇ Th∆∞ m·ª•c d·ª± √°n: {ROOT_DIR}\")\n",
        "print(f\"üìÑ File d·ªØ li·ªáu ƒë·∫ßu v√†o: {INPUT_CSV}\")\n",
        "print(f\"üìÅ Th∆∞ m·ª•c l∆∞u video HARMFUL: {HARMFUL_DIR}\")\n",
        "print(f\"üìÅ Th∆∞ m·ª•c l∆∞u video NOT_HARMFUL: {NOT_HARMFUL_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "bIH_mAeEq2xh",
        "outputId": "820a6c1e-455a-4f88-dc37-6eb2e7eb950d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cookies for tiktok.com found: True\n"
          ]
        }
      ],
      "source": [
        "# ===========================\n",
        "# 2) IMPORT & N·∫†P COOKIES.TXT (Netscape)\n",
        "#    -> KH√îNG g√°n th·∫≥ng v√†o header \"Cookie\"\n",
        "#    -> D√πng MozillaCookieJar + requests.Session\n",
        "# ===========================\n",
        "import os, re, json, time, requests\n",
        "import http.cookiejar as cookielib\n",
        "from urllib.parse import urlparse\n",
        "from datetime import datetime, timezone, timedelta\n",
        "import pandas as pd\n",
        "import yt_dlp\n",
        "\n",
        "COOKIES_PATH = \"cookies.txt\"  # Upload file n√†y l√™n Colab c√πng notebook\n",
        "\n",
        "# Ki·ªÉm tra cookies.txt t·ªìn t·∫°i\n",
        "if not os.path.exists(COOKIES_PATH):\n",
        "    from google.colab import files\n",
        "    print(\"Vui l√≤ng upload cookies.txt (Export t·ª´ tr√¨nh duy·ªát).\")\n",
        "    uploaded = files.upload()\n",
        "    COOKIES_PATH = next(iter(uploaded))\n",
        "\n",
        "# T·∫°o session d√πng cookie Netscape\n",
        "cj = cookielib.MozillaCookieJar()\n",
        "cj.load(COOKIES_PATH, ignore_discard=True, ignore_expires=True)\n",
        "\n",
        "session = requests.Session()\n",
        "session.cookies = cj\n",
        "session.headers.update({\n",
        "    \"User-Agent\": \"Mozilla/5.0\",\n",
        "    \"Referer\": \"https://www.tiktok.com/\"\n",
        "})\n",
        "\n",
        "# Ki·ªÉm tra c√≥ cookie cho tiktok kh√¥ng\n",
        "has_tiktok_cookies = any(\"tiktok.com\" in c.domain for c in session.cookies)\n",
        "print(\"Cookies for tiktok.com found:\", has_tiktok_cookies)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Kmwb6AFSq8SU"
      },
      "outputs": [],
      "source": [
        "# ===========================\n",
        "# 3) H√ÄM H·ªñ TR·ª¢ (oEmbed, parse URL/aweme_id)\n",
        "# ===========================\n",
        "def fetch_oembed(video_url, timeout=12):\n",
        "    \"\"\"L·∫•y metadata oEmbed c∆° b·∫£n c·ªßa video (official).\"\"\"\n",
        "    oembed_url = \"https://www.tiktok.com/oembed\"\n",
        "    try:\n",
        "        r = session.get(oembed_url, params={\"url\": video_url}, timeout=timeout)\n",
        "        if r.status_code == 200:\n",
        "            return r.json()\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(\"oEmbed error:\", e)\n",
        "        return None\n",
        "\n",
        "def extract_aweme_id_from_url(video_url: str):\n",
        "    \"\"\"Parse aweme_id t·ª´ URL d·∫°ng /video/<digits>\"\"\"\n",
        "    m = re.search(r\"/video/(\\d+)\", video_url)\n",
        "    return m.group(1) if m else None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SZMq0CrpsEvs"
      },
      "outputs": [],
      "source": [
        "# ===========================\n",
        "# 4) DOWNLOAD VIDEO B·∫∞NG yt-dlp (d√πng cookiefile + headers)\n",
        "# ===========================\n",
        "def download_tiktok_with_yt_dlp(video_url, out_dir, write_info_json=True, max_filesize=None):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    out_template = os.path.join(out_dir, \"%(id)s.%(ext)s\")\n",
        "\n",
        "    ydl_opts = {\n",
        "        \"outtmpl\": out_template,\n",
        "        \"format\": \"best\",\n",
        "        \"noplaylist\": True,\n",
        "        # D√πng cookiefile + UA/Referer ƒë·ªÉ gi·∫£m l·ªói regional/age/403\n",
        "        \"cookiefile\": COOKIES_PATH,\n",
        "        \"http_headers\": {\n",
        "            \"User-Agent\": \"Mozilla/5.0\",\n",
        "            \"Referer\": video_url\n",
        "        },\n",
        "        # Tr√°nh t·∫£i l·∫°i c√πng video\n",
        "        \"download_archive\": os.path.join(out_dir, \"downloaded.txt\"),\n",
        "    }\n",
        "\n",
        "    if write_info_json:\n",
        "        ydl_opts.update({\n",
        "            \"writedescription\": True,\n",
        "            \"writesubtitles\": False,\n",
        "            \"writeinfojson\": True\n",
        "        })\n",
        "\n",
        "    if max_filesize:\n",
        "        ydl_opts[\"max_filesize\"] = max_filesize\n",
        "\n",
        "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "        info = ydl.extract_info(video_url, download=True)\n",
        "\n",
        "    # L·∫•y info json n·∫øu c√≥\n",
        "    info_json_path = None\n",
        "    if isinstance(info, dict) and \"id\" in info:\n",
        "        candidate = os.path.join(out_dir, f\"{info['id']}.info.json\")\n",
        "        if os.path.exists(candidate):\n",
        "            info_json_path = candidate\n",
        "\n",
        "    if info_json_path:\n",
        "        with open(info_json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            info_data = json.load(f)\n",
        "    else:\n",
        "        info_data = info\n",
        "    return info_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FGcuiIxEsGsd"
      },
      "outputs": [],
      "source": [
        "# ===========================\n",
        "# 5) CRAWL COMMENT (WEB API KH√îNG CH√çNH TH·ª®C) B·∫∞NG session\n",
        "#    L∆∞u √Ω: C√≥ th·ªÉ 403/empty tr√™n Colab d√π c√≥ cookies.\n",
        "# ===========================\n",
        "def fetch_comments_web(aweme_id: str, session: requests.Session, max_comments=200, sleep=1.0):\n",
        "    url = \"https://www.tiktok.com/api/comment/list/\"\n",
        "    cursor, out = 0, []\n",
        "    while len(out) < max_comments:\n",
        "        params = {\"aid\": 1988, \"aweme_id\": aweme_id, \"cursor\": cursor, \"count\": 20}\n",
        "        r = session.get(url, params=params, timeout=20)\n",
        "        if r.status_code != 200:\n",
        "            print(\"HTTP\", r.status_code, r.text[:200])\n",
        "            break\n",
        "        try:\n",
        "            data = r.json()\n",
        "        except Exception as e:\n",
        "            print(\"JSON parse error:\", e, r.text[:200]); break\n",
        "\n",
        "        out.extend(data.get(\"comments\") or [])\n",
        "        print(f\"Fetched {len(out)} comments so far‚Ä¶\")\n",
        "\n",
        "        if not data.get(\"has_more\"):\n",
        "            break\n",
        "        cursor = data.get(\"cursor\", cursor + 20)\n",
        "        if sleep: time.sleep(sleep)\n",
        "    return out\n",
        "\n",
        "def fetch_replies_web(aweme_id: str, comment_id: str, session: requests.Session, max_replies=100, sleep=1.0):\n",
        "    url = \"https://www.tiktok.com/api/comment/list/reply/\"\n",
        "    cursor, out = 0, []\n",
        "    while len(out) < max_replies:\n",
        "        params = {\"aid\": 1988, \"aweme_id\": aweme_id, \"comment_id\": comment_id, \"cursor\": cursor, \"count\": 20}\n",
        "        r = session.get(url, params=params, timeout=20)\n",
        "        if r.status_code != 200:\n",
        "            print(\"HTTP\", r.status_code, r.text[:200])\n",
        "            break\n",
        "        try:\n",
        "            data = r.json()\n",
        "        except Exception as e:\n",
        "            print(\"JSON parse error:\", e, r.text[:200]); break\n",
        "\n",
        "        out.extend(data.get(\"comments\") or [])\n",
        "        if not data.get(\"has_more\"):\n",
        "            break\n",
        "        cursor = data.get(\"cursor\", cursor + 20)\n",
        "        if sleep: time.sleep(sleep)\n",
        "    return out\n",
        "\n",
        "def fetch_all_comments_with_replies_web(aweme_id: str, session: requests.Session,\n",
        "                                        max_comments=200, max_replies_per_comment=50, sleep=1.0):\n",
        "    comments = fetch_comments_web(aweme_id, session, max_comments=max_comments, sleep=sleep)\n",
        "    results = []\n",
        "    for c in comments:\n",
        "        item = {\n",
        "            \"cid\": c.get(\"cid\"),\n",
        "            \"text\": c.get(\"text\"),\n",
        "            \"author\": (c.get(\"user\") or {}).get(\"nickname\"),\n",
        "            \"create_time\": c.get(\"create_time\"),\n",
        "            \"like_count\": c.get(\"digg_count\"),\n",
        "            \"reply_count\": c.get(\"reply_comment_total\"),\n",
        "            \"replies\": []\n",
        "        }\n",
        "        try:\n",
        "            if item[\"cid\"]:\n",
        "                rs = fetch_replies_web(aweme_id, item[\"cid\"], session, max_replies=max_replies_per_comment, sleep=sleep)\n",
        "                item[\"replies\"] = rs\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Reply error for {item['cid']}: {e}\")\n",
        "        results.append(item)\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vs3qqXbcsIiM"
      },
      "outputs": [],
      "source": [
        "# ===========================\n",
        "# 6) H√ÄM CRAWL 1 VIDEO: oEmbed ‚Üí yt-dlp ‚Üí comments ‚Üí JSON\n",
        "#    + Fallback l·∫•y aweme_id n·∫øu oEmbed kh√¥ng c√≥\n",
        "# ===========================\n",
        "def crawl_one_tiktok(video_url, out_dir, use_comments=False, max_comments=200, max_replies_per_comment=50, sleep=1.0):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    result = {\n",
        "        \"url\": video_url,\n",
        "        \"crawled_at\": datetime.utcnow().isoformat() + \"Z\"\n",
        "    }\n",
        "\n",
        "    # 1) oEmbed\n",
        "    oembed = fetch_oembed(video_url)\n",
        "    result[\"oembed\"] = oembed\n",
        "\n",
        "    # 2) T·∫£i video + ƒë·ªçc info\n",
        "    try:\n",
        "        meta = download_tiktok_with_yt_dlp(video_url, out_dir)\n",
        "        result[\"yt_dlp_info\"] = meta\n",
        "    except Exception as e:\n",
        "        result[\"yt_dlp_error\"] = str(e)\n",
        "        meta = None\n",
        "\n",
        "    # 3) L·∫•y aweme_id\n",
        "    aweme_id = None\n",
        "    if oembed and isinstance(oembed, dict):\n",
        "        aweme_id = oembed.get(\"embed_product_id\")\n",
        "    if not aweme_id:\n",
        "        aweme_id = extract_aweme_id_from_url(video_url)\n",
        "    if not aweme_id and isinstance(meta, dict):\n",
        "        aweme_id = str(meta.get(\"id\"))\n",
        "\n",
        "    if not aweme_id:\n",
        "        raise ValueError(\"Kh√¥ng l·∫•y ƒë∆∞·ª£c aweme_id cho video n√†y\")\n",
        "\n",
        "    print(\"aweme_id:\", aweme_id)\n",
        "\n",
        "    # 4) Comments (tu·ª≥ ch·ªçn)\n",
        "    if use_comments:\n",
        "        cmts = fetch_all_comments_with_replies_web(\n",
        "            aweme_id, session, max_comments=max_comments,\n",
        "            max_replies_per_comment=max_replies_per_comment, sleep=sleep\n",
        "        )\n",
        "        result[\"comments\"] = cmts\n",
        "\n",
        "    # 5) Save metadata JSON\n",
        "    json_path = os.path.join(out_dir, f\"{aweme_id}_crawl.json\")\n",
        "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(result, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    return result, json_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===========================\n",
        "# 7) XU·∫§T EXCEL (comments + replies th√†nh 1 sheet)\n",
        "# ===========================\n",
        "# def export_comments_to_excel(all_items, aweme_id: str, out_dir: str):\n",
        "#     rows = []\n",
        "#     for c in all_items or []:\n",
        "#         # Top-level\n",
        "#         rows.append({\n",
        "#             \"video_id\": aweme_id,\n",
        "#             \"cid\": c.get(\"cid\"),\n",
        "#             \"parent_cid\": None,\n",
        "#             \"is_reply\": 0,\n",
        "#             \"author_name\": c.get(\"author\"),\n",
        "#             \"text\": c.get(\"text\"),\n",
        "#             \"like_count\": c.get(\"like_count\"),\n",
        "#             \"reply_count\": c.get(\"reply_count\"),\n",
        "#             \"create_time\": c.get(\"create_time\"),\n",
        "#         })\n",
        "#         # Replies\n",
        "#         for r in (c.get(\"replies\") or []):\n",
        "#             ru = r.get(\"user\") or {}\n",
        "#             rows.append({\n",
        "#                 \"video_id\": aweme_id,\n",
        "#                 \"cid\": r.get(\"cid\"),\n",
        "#                 \"parent_cid\": c.get(\"cid\"),\n",
        "#                 \"is_reply\": 1,\n",
        "#                 \"author_name\": ru.get(\"nickname\"),\n",
        "#                 \"text\": r.get(\"text\"),\n",
        "#                 \"like_count\": r.get(\"digg_count\") or r.get(\"like_count\"),\n",
        "#                 \"reply_count\": r.get(\"reply_comment_total\") or r.get(\"reply_count\"),\n",
        "#                 \"create_time\": r.get(\"create_time\"),\n",
        "#             })\n",
        "\n",
        "#     df = pd.DataFrame(rows)\n",
        "\n",
        "#     # Epoch -> th·ªùi gian\n",
        "#     if \"create_time\" in df.columns:\n",
        "#         dt = pd.to_datetime(df[\"create_time\"], unit=\"s\", errors=\"coerce\", utc=True)\n",
        "#         df[\"created_at_utc\"] = dt\n",
        "#         df[\"created_at_vn\"] = dt.dt.tz_convert(\"Asia/Ho_Chi_Minh\")\n",
        "\n",
        "#     # S·∫Øp c·ªôt\n",
        "#     cols = [\"video_id\",\"cid\",\"parent_cid\",\"is_reply\",\"author_name\",\n",
        "#             \"text\",\"like_count\",\"reply_count\",\"create_time\",\n",
        "#             \"created_at_utc\",\"created_at_vn\"]\n",
        "#     df = df[[c for c in cols if c in df.columns]]\n",
        "\n",
        "#     os.makedirs(out_dir, exist_ok=True)\n",
        "#     xlsx_path = os.path.join(out_dir, f\"{aweme_id}_comments.xlsx\")\n",
        "#     with pd.ExcelWriter(xlsx_path, engine=\"xlsxwriter\") as writer:\n",
        "#         df.to_excel(writer, index=False, sheet_name=\"comments\")\n",
        "#         ws = writer.sheets[\"comments\"]\n",
        "#         ws.freeze_panes(1, 0)\n",
        "#         for i, col in enumerate(df.columns):\n",
        "#             maxlen = min(60, max(10, df[col].astype(str).str.len().max() if not df.empty else 10))\n",
        "#             ws.set_column(i, i, maxlen + 2)\n",
        "#     print(\"‚úÖ Saved Excel:\", xlsx_path)\n",
        "#     return xlsx_path, df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "dgyAjyq3sM_Z"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def export_comments_to_excel(all_items, aweme_id: str, out_dir: str):\n",
        "    import os, pandas as pd\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    rows = []\n",
        "    for c in all_items or []:\n",
        "        # Top-level\n",
        "        rows.append({\n",
        "            \"video_id\": aweme_id,\n",
        "            \"cid\": c.get(\"cid\"),\n",
        "            \"parent_cid\": None,\n",
        "            \"is_reply\": 0,\n",
        "            \"author_name\": c.get(\"author\"),\n",
        "            \"text\": c.get(\"text\"),\n",
        "            \"like_count\": c.get(\"like_count\"),\n",
        "            \"reply_count\": c.get(\"reply_count\"),\n",
        "            \"create_time\": c.get(\"create_time\"),\n",
        "        })\n",
        "        # Replies\n",
        "        for r in (c.get(\"replies\") or []):\n",
        "            ru = r.get(\"user\") or {}\n",
        "            rows.append({\n",
        "                \"video_id\": aweme_id,\n",
        "                \"cid\": r.get(\"cid\"),\n",
        "                \"parent_cid\": c.get(\"cid\"),\n",
        "                \"is_reply\": 1,\n",
        "                \"author_name\": ru.get(\"nickname\"),\n",
        "                \"text\": r.get(\"text\"),\n",
        "                \"like_count\": r.get(\"digg_count\") or r.get(\"like_count\"),\n",
        "                \"reply_count\": r.get(\"reply_comment_total\") or r.get(\"reply_count\"),\n",
        "                \"create_time\": r.get(\"create_time\"),\n",
        "            })\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "\n",
        "    # Epoch -> datetime (tz-aware), r·ªìi b·ªè tz ƒë·ªÉ Excel ch·∫•p nh·∫≠n\n",
        "    if \"create_time\" in df.columns:\n",
        "        dt_utc = pd.to_datetime(df[\"create_time\"], unit=\"s\", errors=\"coerce\", utc=True)\n",
        "        # B·∫¢N NAIVE (kh√¥ng timezone)\n",
        "        df[\"created_at_utc\"] = dt_utc.dt.tz_localize(None)\n",
        "        df[\"created_at_vn\"]  = dt_utc.dt.tz_convert(\"Asia/Ho_Chi_Minh\").dt.tz_localize(None)\n",
        "\n",
        "    # S·∫Øp c·ªôt\n",
        "    cols = [\"video_id\",\"cid\",\"parent_cid\",\"is_reply\",\"author_name\",\n",
        "            \"text\",\"like_count\",\"reply_count\",\"create_time\",\n",
        "            \"created_at_utc\",\"created_at_vn\"]\n",
        "    df = df[[c for c in cols if c in df.columns]]\n",
        "\n",
        "    xlsx_path = os.path.join(out_dir, f\"{aweme_id}_comments.xlsx\")\n",
        "    with pd.ExcelWriter(xlsx_path, engine=\"xlsxwriter\",\n",
        "                        datetime_format=\"yyyy-mm-dd hh:mm:ss\") as writer:\n",
        "        df.to_excel(writer, index=False, sheet_name=\"comments\")\n",
        "        ws = writer.sheets[\"comments\"]\n",
        "        ws.freeze_panes(1, 0)\n",
        "        # Auto-width\n",
        "        for i, col in enumerate(df.columns):\n",
        "            maxlen = min(60, max(10, df[col].astype(str).str.len().max() if not df.empty else 10))\n",
        "            ws.set_column(i, i, maxlen + 2)\n",
        "\n",
        "    print(\"‚úÖ Saved Excel:\", xlsx_path)\n",
        "    return xlsx_path, df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ƒêang ƒë·ªçc d·ªØ li·ªáu t·ª´: /home/guest/Projects/SE363/-UIT-_SE363-Big-Data-Platform-Application-Development/data/crawl/sub_tiktok_links.csv ...\n",
            "-> T·ªïng s·ªë d√≤ng trong CSV: 1000\n",
            "-> S·ªë l∆∞·ª£ng VIDEO th·ª±c t·∫ø c·∫ßn crawl: 507\n",
            "=======================================================\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c356531ef9ef487a941c59f8940d026a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "T·ªïng ti·∫øn ƒë·ªô:   0%|          | 0/507 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚ñ∂Ô∏è ƒêang x·ª≠ l√Ω: https://www.tiktok.com/@trangsachhaytv/video/7259382494527622406\n",
            "   üè∑Ô∏è Nh√£n: NOT_HARMFUL | #Ô∏è‚É£ Hashtag: #sachhay\n",
            "   üìÅ T√™n th∆∞ m·ª•c: video_1 (ID: 7259382494527622406)\n",
            "   ‚¨áÔ∏è B·∫Øt ƒë·∫ßu crawl v√†o: /home/guest/Projects/SE363/-UIT-_SE363-Big-Data-Platform-Application-Development/data/videos/not_harmful/video_1 ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_628607/2892349339.py:9: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  \"crawled_at\": datetime.utcnow().isoformat() + \"Z\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TikTok] Extracting URL: https://www.tiktok.com/@trangsachhaytv/video/7259382494527622406\n",
            "[TikTok] 7259382494527622406: Downloading webpage\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: [TikTok] The extractor is attempting impersonation, but no impersonate target is available. If you encounter errors, then see  https://github.com/yt-dlp/yt-dlp#impersonation  for information on installing the required dependencies\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[info] 7259382494527622406: Downloading 1 format(s): bytevc1_1080p_656162-1\n",
            "[info] Writing video description to: /home/guest/Projects/SE363/-UIT-_SE363-Big-Data-Platform-Application-Development/data/videos/not_harmful/video_1/7259382494527622406.description\n",
            "[info] Writing video metadata as JSON to: /home/guest/Projects/SE363/-UIT-_SE363-Big-Data-Platform-Application-Development/data/videos/not_harmful/video_1/7259382494527622406.info.json\n",
            "[download] Destination: /home/guest/Projects/SE363/-UIT-_SE363-Big-Data-Platform-Application-Development/data/videos/not_harmful/video_1/7259382494527622406.mp4\n",
            "[download] 100% of    2.17MiB in 00:00:01 at 1.61MiB/s     \n",
            "aweme_id: 7259382494527622406\n",
            "Fetched 19 comments so far‚Ä¶\n",
            "Fetched 38 comments so far‚Ä¶\n",
            "Fetched 57 comments so far‚Ä¶\n",
            "Fetched 77 comments so far‚Ä¶\n",
            "Fetched 97 comments so far‚Ä¶\n",
            "Fetched 117 comments so far‚Ä¶\n",
            "Fetched 136 comments so far‚Ä¶\n",
            "Fetched 156 comments so far‚Ä¶\n",
            "Fetched 175 comments so far‚Ä¶\n",
            "Fetched 190 comments so far‚Ä¶\n",
            "   ‚úÖ Metadata JSON saved: 7259382494527622406_crawl.json\n",
            "‚úÖ Saved Excel: /home/guest/Projects/SE363/-UIT-_SE363-Big-Data-Platform-Application-Development/data/videos/not_harmful/video_1/7259382494527622406_comments.xlsx\n",
            "   ‚úÖ Comments Excel saved: 7259382494527622406_comments.xlsx (190 cmts)\n",
            "   üí§ ƒêang ngh·ªâ 5s...\n",
            "\n",
            "‚ñ∂Ô∏è ƒêang x·ª≠ l√Ω: https://www.tiktok.com/@seoitama/video/7504544082270752007\n",
            "   üè∑Ô∏è Nh√£n: HARMFUL | #Ô∏è‚É£ Hashtag: #bikini\n",
            "   üìÅ T√™n th∆∞ m·ª•c: video_1 (ID: 7504544082270752007)\n",
            "   ‚¨áÔ∏è B·∫Øt ƒë·∫ßu crawl v√†o: /home/guest/Projects/SE363/-UIT-_SE363-Big-Data-Platform-Application-Development/data/videos/harmful/video_1 ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_628607/2892349339.py:9: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  \"crawled_at\": datetime.utcnow().isoformat() + \"Z\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TikTok] Extracting URL: https://www.tiktok.com/@seoitama/video/7504544082270752007\n",
            "[TikTok] 7504544082270752007: Downloading webpage\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: [TikTok] The extractor is attempting impersonation, but no impersonate target is available. If you encounter errors, then see  https://github.com/yt-dlp/yt-dlp#impersonation  for information on installing the required dependencies\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[info] 7504544082270752007: Downloading 1 format(s): bytevc1_540p_421352-1\n",
            "[info] Writing video description to: /home/guest/Projects/SE363/-UIT-_SE363-Big-Data-Platform-Application-Development/data/videos/harmful/video_1/7504544082270752007.description\n",
            "[info] Writing video metadata as JSON to: /home/guest/Projects/SE363/-UIT-_SE363-Big-Data-Platform-Application-Development/data/videos/harmful/video_1/7504544082270752007.info.json\n",
            "[download] Destination: /home/guest/Projects/SE363/-UIT-_SE363-Big-Data-Platform-Application-Development/data/videos/harmful/video_1/7504544082270752007.mp4\n",
            "[download] 100% of  694.01KiB in 00:00:00 at 2.30MiB/s     \n",
            "aweme_id: 7504544082270752007\n",
            "Fetched 20 comments so far‚Ä¶\n",
            "Fetched 40 comments so far‚Ä¶\n",
            "Fetched 59 comments so far‚Ä¶\n",
            "Fetched 79 comments so far‚Ä¶\n",
            "Fetched 99 comments so far‚Ä¶\n",
            "Fetched 119 comments so far‚Ä¶\n",
            "Fetched 139 comments so far‚Ä¶\n",
            "Fetched 159 comments so far‚Ä¶\n",
            "Fetched 179 comments so far‚Ä¶\n",
            "Fetched 199 comments so far‚Ä¶\n",
            "Fetched 219 comments so far‚Ä¶\n",
            "   ‚úÖ Metadata JSON saved: 7504544082270752007_crawl.json\n",
            "‚úÖ Saved Excel: /home/guest/Projects/SE363/-UIT-_SE363-Big-Data-Platform-Application-Development/data/videos/harmful/video_1/7504544082270752007_comments.xlsx\n",
            "   ‚úÖ Comments Excel saved: 7504544082270752007_comments.xlsx (219 cmts)\n",
            "   üí§ ƒêang ngh·ªâ 5s...\n",
            "\n",
            "‚ñ∂Ô∏è ƒêang x·ª≠ l√Ω: https://www.tiktok.com/@sedyyfilms/video/7537710270127328534\n",
            "   üè∑Ô∏è Nh√£n: NOT_HARMFUL | #Ô∏è‚É£ Hashtag: #inspiration\n",
            "   üìÅ T√™n th∆∞ m·ª•c: video_2 (ID: 7537710270127328534)\n",
            "   ‚¨áÔ∏è B·∫Øt ƒë·∫ßu crawl v√†o: /home/guest/Projects/SE363/-UIT-_SE363-Big-Data-Platform-Application-Development/data/videos/not_harmful/video_2 ...\n",
            "[TikTok] Extracting URL: https://www.tiktok.com/@sedyyfilms/video/7537710270127328534\n",
            "[TikTok] 7537710270127328534: Downloading webpage\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_628607/2892349339.py:9: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  \"crawled_at\": datetime.utcnow().isoformat() + \"Z\"\n",
            "WARNING: [TikTok] The extractor is attempting impersonation, but no impersonate target is available. If you encounter errors, then see  https://github.com/yt-dlp/yt-dlp#impersonation  for information on installing the required dependencies\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[info] 7537710270127328534: Downloading 1 format(s): bytevc1_1080p_1453930-1\n",
            "[info] Writing video description to: /home/guest/Projects/SE363/-UIT-_SE363-Big-Data-Platform-Application-Development/data/videos/not_harmful/video_2/7537710270127328534.description\n",
            "[info] Writing video metadata as JSON to: /home/guest/Projects/SE363/-UIT-_SE363-Big-Data-Platform-Application-Development/data/videos/not_harmful/video_2/7537710270127328534.info.json\n",
            "[download] Destination: /home/guest/Projects/SE363/-UIT-_SE363-Big-Data-Platform-Application-Development/data/videos/not_harmful/video_2/7537710270127328534.mp4\n",
            "[download] 100% of    2.06MiB in 00:00:01 at 1.49MiB/s     \n",
            "aweme_id: 7537710270127328534\n",
            "Fetched 20 comments so far‚Ä¶\n",
            "Fetched 40 comments so far‚Ä¶\n",
            "Fetched 60 comments so far‚Ä¶\n",
            "Fetched 80 comments so far‚Ä¶\n",
            "Fetched 99 comments so far‚Ä¶\n",
            "Fetched 119 comments so far‚Ä¶\n",
            "Fetched 139 comments so far‚Ä¶\n",
            "Fetched 157 comments so far‚Ä¶\n",
            "Fetched 176 comments so far‚Ä¶\n",
            "Fetched 195 comments so far‚Ä¶\n",
            "Fetched 199 comments so far‚Ä¶\n",
            "   ‚úÖ Metadata JSON saved: 7537710270127328534_crawl.json\n",
            "‚úÖ Saved Excel: /home/guest/Projects/SE363/-UIT-_SE363-Big-Data-Platform-Application-Development/data/videos/not_harmful/video_2/7537710270127328534_comments.xlsx\n",
            "   ‚úÖ Comments Excel saved: 7537710270127328534_comments.xlsx (199 cmts)\n",
            "   üí§ ƒêang ngh·ªâ 5s...\n",
            "\n",
            "‚ñ∂Ô∏è ƒêang x·ª≠ l√Ω: https://www.tiktok.com/@nihao.trip/video/7568753395637521672\n",
            "   üè∑Ô∏è Nh√£n: NOT_HARMFUL | #Ô∏è‚É£ Hashtag: #travel\n",
            "   üìÅ T√™n th∆∞ m·ª•c: video_3 (ID: 7568753395637521672)\n",
            "   ‚¨áÔ∏è B·∫Øt ƒë·∫ßu crawl v√†o: /home/guest/Projects/SE363/-UIT-_SE363-Big-Data-Platform-Application-Development/data/videos/not_harmful/video_3 ...\n",
            "[TikTok] Extracting URL: https://www.tiktok.com/@nihao.trip/video/7568753395637521672\n",
            "[TikTok] 7568753395637521672: Downloading webpage\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_628607/2892349339.py:9: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  \"crawled_at\": datetime.utcnow().isoformat() + \"Z\"\n",
            "WARNING: [TikTok] The extractor is attempting impersonation, but no impersonate target is available. If you encounter errors, then see  https://github.com/yt-dlp/yt-dlp#impersonation  for information on installing the required dependencies\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[info] 7568753395637521672: Downloading 1 format(s): bytevc1_1080p_2053282-1\n",
            "[info] Writing video description to: /home/guest/Projects/SE363/-UIT-_SE363-Big-Data-Platform-Application-Development/data/videos/not_harmful/video_3/7568753395637521672.description\n",
            "[info] Writing video metadata as JSON to: /home/guest/Projects/SE363/-UIT-_SE363-Big-Data-Platform-Application-Development/data/videos/not_harmful/video_3/7568753395637521672.info.json\n",
            "[download] Destination: /home/guest/Projects/SE363/-UIT-_SE363-Big-Data-Platform-Application-Development/data/videos/not_harmful/video_3/7568753395637521672.mp4\n",
            "[download] 100% of    3.38MiB in 00:00:00 at 8.09MiB/s     \n",
            "aweme_id: 7568753395637521672\n",
            "Fetched 20 comments so far‚Ä¶\n",
            "Fetched 40 comments so far‚Ä¶\n",
            "Fetched 60 comments so far‚Ä¶\n",
            "Fetched 80 comments so far‚Ä¶\n",
            "Fetched 100 comments so far‚Ä¶\n",
            "Fetched 120 comments so far‚Ä¶\n",
            "Fetched 140 comments so far‚Ä¶\n",
            "Fetched 160 comments so far‚Ä¶\n",
            "Fetched 180 comments so far‚Ä¶\n",
            "Fetched 200 comments so far‚Ä¶\n",
            "   ‚úÖ Metadata JSON saved: 7568753395637521672_crawl.json\n",
            "‚úÖ Saved Excel: /home/guest/Projects/SE363/-UIT-_SE363-Big-Data-Platform-Application-Development/data/videos/not_harmful/video_3/7568753395637521672_comments.xlsx\n",
            "   ‚úÖ Comments Excel saved: 7568753395637521672_comments.xlsx (200 cmts)\n",
            "   üí§ ƒêang ngh·ªâ 5s...\n",
            "\n",
            "‚ñ∂Ô∏è ƒêang x·ª≠ l√Ω: https://www.tiktok.com/@rafournine/video/7491606076522106118\n",
            "   üè∑Ô∏è Nh√£n: HARMFUL | #Ô∏è‚É£ Hashtag: #drama\n",
            "   üìÅ T√™n th∆∞ m·ª•c: video_2 (ID: 7491606076522106118)\n",
            "   ‚¨áÔ∏è B·∫Øt ƒë·∫ßu crawl v√†o: /home/guest/Projects/SE363/-UIT-_SE363-Big-Data-Platform-Application-Development/data/videos/harmful/video_2 ...\n",
            "[TikTok] Extracting URL: https://www.tiktok.com/@rafournine/video/7491606076522106118\n",
            "[TikTok] 7491606076522106118: Downloading webpage\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_628607/2892349339.py:9: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  \"crawled_at\": datetime.utcnow().isoformat() + \"Z\"\n",
            "WARNING: [TikTok] The extractor is attempting impersonation, but no impersonate target is available. If you encounter errors, then see  https://github.com/yt-dlp/yt-dlp#impersonation  for information on installing the required dependencies\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[info] 7491606076522106118: Downloading 1 format(s): bytevc1_720p_94949-1\n",
            "[info] Writing video description to: /home/guest/Projects/SE363/-UIT-_SE363-Big-Data-Platform-Application-Development/data/videos/harmful/video_2/7491606076522106118.description\n",
            "[info] Writing video metadata as JSON to: /home/guest/Projects/SE363/-UIT-_SE363-Big-Data-Platform-Application-Development/data/videos/harmful/video_2/7491606076522106118.info.json\n",
            "[download] Destination: /home/guest/Projects/SE363/-UIT-_SE363-Big-Data-Platform-Application-Development/data/videos/harmful/video_2/7491606076522106118.mp4\n",
            "[download] 100% of  423.08KiB in 00:00:01 at 338.99KiB/s   \n",
            "aweme_id: 7491606076522106118\n",
            "Fetched 20 comments so far‚Ä¶\n",
            "Fetched 40 comments so far‚Ä¶\n",
            "Fetched 60 comments so far‚Ä¶\n",
            "Fetched 80 comments so far‚Ä¶\n",
            "Fetched 100 comments so far‚Ä¶\n",
            "Fetched 120 comments so far‚Ä¶\n",
            "Fetched 140 comments so far‚Ä¶\n",
            "Fetched 160 comments so far‚Ä¶\n",
            "Fetched 180 comments so far‚Ä¶\n",
            "Fetched 200 comments so far‚Ä¶\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 81\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   ‚¨áÔ∏è B·∫Øt ƒë·∫ßu crawl v√†o: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_specific_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     80\u001b[39m \u001b[38;5;66;03m# --- G·ªåI H√ÄM CRAWL G·ªêC ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m res, jsonp = \u001b[43mcrawl_one_tiktok\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m    \u001b[49m\u001b[43moriginal_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvideo_specific_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_comments\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# L·∫•y comment\u001b[39;49;00m\n\u001b[32m     85\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_comments\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# TƒÉng s·ªë l∆∞·ª£ng n·∫øu c·∫ßn\u001b[39;49;00m\n\u001b[32m     86\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_replies_per_comment\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m    \u001b[49m\u001b[43msleep\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m                   \u001b[49m\u001b[38;5;66;43;03m# Ngh·ªâ gi·ªØa c√°c l·∫ßn g·ªçi API comment\u001b[39;49;00m\n\u001b[32m     88\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   ‚úÖ Metadata JSON saved: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos.path.basename(jsonp)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# --- XU·∫§T EXCEL COMMENT ---\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mcrawl_one_tiktok\u001b[39m\u001b[34m(video_url, out_dir, use_comments, max_comments, max_replies_per_comment, sleep)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# 4) Comments (tu·ª≥ ch·ªçn)\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_comments:\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     cmts = \u001b[43mfetch_all_comments_with_replies_web\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m        \u001b[49m\u001b[43maweme_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_comments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_comments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_replies_per_comment\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_replies_per_comment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msleep\u001b[49m\u001b[43m=\u001b[49m\u001b[43msleep\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m     result[\u001b[33m\"\u001b[39m\u001b[33mcomments\u001b[39m\u001b[33m\"\u001b[39m] = cmts\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# 5) Save metadata JSON\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 65\u001b[39m, in \u001b[36mfetch_all_comments_with_replies_web\u001b[39m\u001b[34m(aweme_id, session, max_comments, max_replies_per_comment, sleep)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     64\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m item[\u001b[33m\"\u001b[39m\u001b[33mcid\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m         rs = \u001b[43mfetch_replies_web\u001b[49m\u001b[43m(\u001b[49m\u001b[43maweme_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcid\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_replies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_replies_per_comment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msleep\u001b[49m\u001b[43m=\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m         item[\u001b[33m\"\u001b[39m\u001b[33mreplies\u001b[39m\u001b[33m\"\u001b[39m] = rs\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mfetch_replies_web\u001b[39m\u001b[34m(aweme_id, comment_id, session, max_replies, sleep)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < max_replies:\n\u001b[32m     32\u001b[39m     params = {\u001b[33m\"\u001b[39m\u001b[33maid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m1988\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33maweme_id\u001b[39m\u001b[33m\"\u001b[39m: aweme_id, \u001b[33m\"\u001b[39m\u001b[33mcomment_id\u001b[39m\u001b[33m\"\u001b[39m: comment_id, \u001b[33m\"\u001b[39m\u001b[33mcursor\u001b[39m\u001b[33m\"\u001b[39m: cursor, \u001b[33m\"\u001b[39m\u001b[33mcount\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m20\u001b[39m}\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     r = \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m r.status_code != \u001b[32m200\u001b[39m:\n\u001b[32m     35\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mHTTP\u001b[39m\u001b[33m\"\u001b[39m, r.status_code, r.text[:\u001b[32m200\u001b[39m])\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/SE363/lib/python3.12/site-packages/requests/sessions.py:602\u001b[39m, in \u001b[36mSession.get\u001b[39m\u001b[34m(self, url, **kwargs)\u001b[39m\n\u001b[32m    594\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[32m    595\u001b[39m \n\u001b[32m    596\u001b[39m \u001b[33;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m    597\u001b[39m \u001b[33;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[32m    598\u001b[39m \u001b[33;03m:rtype: requests.Response\u001b[39;00m\n\u001b[32m    599\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    601\u001b[39m kwargs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m602\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGET\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/SE363/lib/python3.12/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/SE363/lib/python3.12/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/SE363/lib/python3.12/site-packages/requests/adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    641\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/SE363/lib/python3.12/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/SE363/lib/python3.12/site-packages/urllib3/connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    536\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/SE363/lib/python3.12/site-packages/urllib3/connection.py:565\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    562\u001b[39m _shutdown = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.sock, \u001b[33m\"\u001b[39m\u001b[33mshutdown\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    568\u001b[39m     assert_header_parsing(httplib_response.msg)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/SE363/lib/python3.12/http/client.py:1430\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1428\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1429\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1430\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1431\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1432\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/SE363/lib/python3.12/http/client.py:331\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    333\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/SE363/lib/python3.12/http/client.py:292\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    294\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/SE363/lib/python3.12/socket.py:720\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    721\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    722\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/SE363/lib/python3.12/ssl.py:1251\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1247\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1248\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1249\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1250\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1252\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1253\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/SE363/lib/python3.12/ssl.py:1103\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1101\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1102\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1104\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1105\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# ===========================\n",
        "# 8) CH·∫†Y T·ª∞ ƒê·ªòNG T·ª™ FILE CSV (C√ì PH√ÇN LO·∫†I & RESUME)\n",
        "# ===========================\n",
        "import pandas as pd\n",
        "import time\n",
        "import os\n",
        "from tqdm.notebook import tqdm # B·ªè comment n·∫øu mu·ªën d√πng thanh ti·∫øn tr√¨nh ƒë·∫πp h∆°n\n",
        "\n",
        "# 1. ƒê·ªçc v√† L·ªçc file CSV\n",
        "if not os.path.exists(INPUT_CSV):\n",
        "    print(f\"‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y file {INPUT_CSV}\")\n",
        "    print(\"Vui l√≤ng ch·∫°y script 'find_tiktok_links.py' tr∆∞·ªõc ƒë·ªÉ c√≥ d·ªØ li·ªáu.\")\n",
        "else:\n",
        "    print(f\"ƒêang ƒë·ªçc d·ªØ li·ªáu t·ª´: {INPUT_CSV} ...\")\n",
        "    df = pd.read_csv(INPUT_CSV)\n",
        "    print(f\"-> T·ªïng s·ªë d√≤ng trong CSV: {len(df)}\")\n",
        "\n",
        "    # L·ªåC QUAN TR·ªåNG: Ch·ªâ l·∫•y nh·ªØng link l√† VIDEO (c√≥ ch·ª©a '/video/')\n",
        "    # Lo·∫°i b·ªè c√°c link profile (v√≠ d·ª•: tiktok.com/@username)\n",
        "    # L√†m s·∫°ch link (b·ªè c√°c ph·∫ßn th·ª´a sau d·∫•u ?)\n",
        "    # Lo·∫°i b·ªè c√°c link tr√πng l·∫∑p\n",
        "    df_videos = df[df['link'].str.contains('/video/', na=False)].copy()\n",
        "    df_videos['link'] = df_videos['link'].apply(lambda x: x.split('?')[0])\n",
        "    df_videos = df_videos.drop_duplicates(subset=['link'])\n",
        "\n",
        "    print(f\"-> S·ªë l∆∞·ª£ng VIDEO th·ª±c t·∫ø c·∫ßn crawl: {len(df_videos)}\")\n",
        "    print(\"=======================================================\")\n",
        "    \n",
        "    # Ch√∫ng ta c·∫ßn 2 bi·∫øn ƒë·∫øm ri√™ng bi·ªát ƒë·ªÉ ƒë·∫£m b·∫£o c·∫£ 2 th∆∞ m·ª•c\n",
        "    # ƒë·ªÅu b·∫Øt ƒë·∫ßu t·ª´ video_1\n",
        "    harmful_counter = 1\n",
        "    not_harmful_counter = 1\n",
        "\n",
        "    # 2. V√≤ng l·∫∑p Crawl ch√≠nh\n",
        "    # (D√πng itertuples ƒë·ªÉ l·∫∑p qua dataframe nhanh h∆°n)\n",
        "    # (D√πng tqdm b·ªçc df_videos.itertuples() ƒë·ªÉ c√≥ thanh ti·∫øn tr√¨nh)\n",
        "    for row in tqdm(df_videos.itertuples(index=False), total=len(df_videos), desc=\"T·ªïng ti·∫øn ƒë·ªô\"):\n",
        "        original_url = row.link\n",
        "        label = row.label\n",
        "        hashtag = row.hashtag\n",
        "        \n",
        "        # a) L·∫•y ID video (v·∫´n c·∫ßn thi·∫øt ƒë·ªÉ l√†m t√™n file v√† crawl API)\n",
        "        aweme_id = extract_aweme_id_from_url(original_url)\n",
        "        if not aweme_id:\n",
        "            print(f\"\\n   ‚ö†Ô∏è Kh√¥ng l·∫•y ƒë∆∞·ª£c ID video t·ª´ URL: {original_url}. B·ªè qua.\")\n",
        "            continue\n",
        "\n",
        "        # --- THAY ƒê·ªîI (v3.17): Quy·∫øt ƒë·ªãnh t√™n th∆∞ m·ª•c ---\n",
        "        folder_name = \"\"\n",
        "        if label == 'harmful':\n",
        "            base_save_dir = HARMFUL_DIR\n",
        "            folder_name = f\"video_{harmful_counter}\"\n",
        "            harmful_counter += 1\n",
        "        else:\n",
        "            base_save_dir = NOT_HARMFUL_DIR\n",
        "            folder_name = f\"video_{not_harmful_counter}\"\n",
        "            not_harmful_counter += 1\n",
        "        # ----------------------------------------------\n",
        "\n",
        "        print(f\"\\n‚ñ∂Ô∏è ƒêang x·ª≠ l√Ω: {original_url}\")\n",
        "        print(f\"   üè∑Ô∏è Nh√£n: {label.upper()} | #Ô∏è‚É£ Hashtag: #{hashtag}\")\n",
        "        print(f\"   üìÅ T√™n th∆∞ m·ª•c: {folder_name} (ID: {aweme_id})\")\n",
        "\n",
        "        # c) T·∫°o th∆∞ m·ª•c ri√™ng cho video n√†y (d√πng folder_name m·ªõi)\n",
        "        # C·∫•u tr√∫c: data/videos/harmful/video_1/\n",
        "        video_specific_dir = os.path.join(base_save_dir, folder_name)\n",
        "\n",
        "        # d) Ki·ªÉm tra RESUME (Ki·ªÉm tra th∆∞ m·ª•c folder_name)\n",
        "        if os.path.exists(video_specific_dir) and os.listdir(video_specific_dir):\n",
        "            print(f\"   ‚è≠Ô∏è Th∆∞ m·ª•c '{folder_name}' ƒë√£ t·ªìn t·∫°i d·ªØ li·ªáu. B·ªè qua (Resume).\")\n",
        "            continue\n",
        "\n",
        "        # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥\n",
        "        os.makedirs(video_specific_dir, exist_ok=True)\n",
        "\n",
        "        # e) B·∫Øt ƒë·∫ßu Crawl (G·ªçi l·∫°i h√†m crawl_one_tiktok c·ªßa b·∫°n)\n",
        "        try:\n",
        "            print(f\"   ‚¨áÔ∏è B·∫Øt ƒë·∫ßu crawl v√†o: {video_specific_dir} ...\")\n",
        "\n",
        "            # --- G·ªåI H√ÄM CRAWL G·ªêC ---\n",
        "            res, jsonp = crawl_one_tiktok(\n",
        "                original_url,\n",
        "                video_specific_dir,\n",
        "                use_comments=True,          # L·∫•y comment\n",
        "                max_comments=200,           # TƒÉng s·ªë l∆∞·ª£ng n·∫øu c·∫ßn\n",
        "                max_replies_per_comment=50,\n",
        "                sleep=1.0                   # Ngh·ªâ gi·ªØa c√°c l·∫ßn g·ªçi API comment\n",
        "            )\n",
        "            print(f\"   ‚úÖ Metadata JSON saved: {os.path.basename(jsonp)}\")\n",
        "\n",
        "            # --- XU·∫§T EXCEL COMMENT ---\n",
        "            cmts = res.get(\"comments\", [])\n",
        "            if cmts:\n",
        "                # G·ªçi h√†m export_comments_to_excel g·ªëc\n",
        "                xlsx_path, _ = export_comments_to_excel(cmts, aweme_id, video_specific_dir)\n",
        "                if xlsx_path:\n",
        "                    print(f\"   ‚úÖ Comments Excel saved: {os.path.basename(xlsx_path)} ({len(cmts)} cmts)\")\n",
        "            else:\n",
        "                print(\"   ‚ÑπÔ∏è Kh√¥ng t√¨m th·∫•y comment n√†o.\")\n",
        "\n",
        "        except Exception as e:\n",
        "             print(f\"   ‚ùå L·ªñI KHI CRAWL VIDEO N√ÄY: {e}\")\n",
        "             # C√≥ th·ªÉ x√≥a th∆∞ m·ª•c l·ªói n·∫øu mu·ªën s·∫°ch s·∫Ω\n",
        "             # import shutil\n",
        "             # shutil.rmtree(video_specific_dir, ignore_errors=True)\n",
        "\n",
        "        # f) Ngh·ªâ ng∆°i ƒë·ªÉ tr√°nh b·ªã ch·∫∑n (Rate Limit)\n",
        "        print(\"   üí§ ƒêang ngh·ªâ 5s...\")\n",
        "        time.sleep(5)\n",
        "\n",
        "    print(\"\\nüéâüéâüéâ ƒê√É HO√ÄN T·∫§T TO√ÄN B·ªò DANH S√ÅCH VIDEO! üéâüéâüéâ\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVKV4yRAsR1V",
        "outputId": "feefc078-a1e7-42bf-b5c8-91a01dd0d16a"
      },
      "outputs": [],
      "source": [
        "# # ===========================\n",
        "# # 8) CH·∫†Y T·ª∞ ƒê·ªòNG T·ª™ FILE TXT (C√ì C∆† CH·∫æ RESUME)\n",
        "# # ===========================\n",
        "# import time\n",
        "# import os # ƒê·∫£m b·∫£o os ƒë√£ ƒë∆∞·ª£c import (m·∫∑c d√π ƒë√£ c√≥ ·ªü √¥ 3)\n",
        "\n",
        "# URL_FILE_PATH = \"urls.txt\" # T√™n file b·∫°n ƒë√£ upload\n",
        "\n",
        "# # --- ƒê·ªçc file urls.txt ---\n",
        "# try:\n",
        "#     with open(URL_FILE_PATH, 'r') as f:\n",
        "#         VIDEO_URL_LIST = [line.strip() for line in f if line.strip() and line.strip().startswith(\"http\")]\n",
        "    \n",
        "#     if not VIDEO_URL_LIST:\n",
        "#         print(f\"L·ªói: File {URL_FILE_PATH} tr·ªëng ho·∫∑c kh√¥ng ch·ª©a link h·ª£p l·ªá.\")\n",
        "#     else:\n",
        "#         print(f\"*** ƒê√£ t√¨m th·∫•y {len(VIDEO_URL_LIST)} URLs trong file. B·∫Øt ƒë·∫ßu crawl... ***\")\n",
        "\n",
        "# except FileNotFoundError:\n",
        "#     print(f\"L·ªói: Kh√¥ng t√¨m th·∫•y file {URL_FILE_PATH}. B·∫°n ƒë√£ upload file ch∆∞a?\")\n",
        "#     VIDEO_URL_LIST = []\n",
        "# # -----------------------------\n",
        "\n",
        "\n",
        "# # D√πng enumerate ƒë·ªÉ l·∫•y c·∫£ index (0, 1, 2...) v√† url\n",
        "# for index, url in enumerate(VIDEO_URL_LIST):\n",
        "    \n",
        "#     # T·∫°o t√™n th∆∞ m·ª•c, v√≠ d·ª•: \"vid_1\", \"vid_2\"...\n",
        "#     folder_name = f\"vid_{index + 1}\"\n",
        "    \n",
        "#     # T·∫°o ƒë∆∞·ªùng d·∫´n ƒë·∫ßy ƒë·ªß ƒë·∫øn th∆∞ m·ª•c m·ªõi n√†y\n",
        "#     video_specific_dir = os.path.join(CRAWL_DIR, folder_name)\n",
        "    \n",
        "#     # -------- C∆† CH·∫æ RESUME (PH·∫¶N TH√äM M·ªöI) --------\n",
        "#     # Ki·ªÉm tra xem th∆∞ m·ª•c n√†y ƒë√£ t·ªìn t·∫°i hay ch∆∞a\n",
        "#     if os.path.exists(video_specific_dir):\n",
        "#         # N·∫øu ƒë√£ t·ªìn t·∫°i, in th√¥ng b√°o v√† b·ªè qua (continue)\n",
        "#         print(f\"\\n--- [B·ªé QUA] Th∆∞ m·ª•c '{folder_name}' ƒë√£ t·ªìn t·∫°i. Chuy·ªÉn sang video ti·∫øp theo. ---\")\n",
        "#         continue # L·ªánh m·∫•u ch·ªët: D·ª´ng v√≤ng l·∫∑p n√†y, ƒëi ƒë·∫øn video ti·∫øp theo\n",
        "#     # -----------------------------------------------\n",
        "\n",
        "#     # N·∫øu th∆∞ m·ª•c ch∆∞a t·ªìn t·∫°i, T·∫†O M·ªöI v√† b·∫Øt ƒë·∫ßu crawl\n",
        "#     os.makedirs(video_specific_dir, exist_ok=True) \n",
        "\n",
        "#     print(f\"\\n=======================================================\")\n",
        "#     print(f\"--- [B·∫ÆT ƒê·∫¶U] X·ª≠ l√Ω: {url} (L∆∞u v√†o: {video_specific_dir}) ---\")\n",
        "    \n",
        "#     try:\n",
        "#         # G·ªçi h√†m crawl v√† TRUY·ªÄN TH∆Ø M·ª§C M·ªöI v√†o:\n",
        "#         res, jsonp = crawl_one_tiktok(\n",
        "#             url, video_specific_dir, \n",
        "#             use_comments=True,         \n",
        "#             max_comments=100,          \n",
        "#             max_replies_per_comment=50 \n",
        "#         )\n",
        "#         print(f\"‚úÖ ƒê√£ l∆∞u Metadata JSON v√†o: {jsonp}\")\n",
        "\n",
        "#         # N·∫øu c√≥ comments th√¨ xu·∫•t Excel\n",
        "#         cmts = res.get(\"comments\", [])\n",
        "#         if cmts:\n",
        "#             aweme_id = extract_aweme_id_from_url(url) or (res.get(\"oembed\") or {}).get(\"embed_product_id\")\n",
        "#             if aweme_id:\n",
        "#                 xlsx_path, df_preview = export_comments_to_excel(cmts, aweme_id, video_specific_dir) \n",
        "#                 print(f\"‚úÖ ƒê√£ l∆∞u Comments Excel v√†o: {xlsx_path}\")\n",
        "#             else:\n",
        "#                  print(\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y aweme_id, b·ªè qua xu·∫•t Excel.\")\n",
        "#         else:\n",
        "#             print(\"‚ÑπÔ∏è Video n√†y kh√¥ng c√≥ comment (ho·∫∑c crawl comment th·∫•t b·∫°i).\")\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"!!!!!!!! L·ªñI NGHI√äM TR·ªåNG !!!!!!!!\")\n",
        "#         print(f\"L·ªói khi x·ª≠ l√Ω {url}: {e}\")\n",
        "#         print(f\"--- [B·ªé QUA] Video n√†y v√† ti·∫øp t·ª•c. ---\")\n",
        "\n",
        "#     print(f\"=======================================================\")\n",
        "    \n",
        "#     time.sleep(5)\n",
        "\n",
        "# print(\"\\n*** ƒê√É HO√ÄN T·∫§T TO√ÄN B·ªò DANH S√ÅCH! ***\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "SE363",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
